{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing dependencies"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from transformers import *\nimport numpy as np\nimport tensorflow as tf\nimport keras\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nimport random\nimport tensorflow.keras.backend as K\nimport sentencepiece as spm\nfrom tensorflow.keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading train csv file"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna(' ')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## getting XLnet and alBERT\nI am using same tokenizer for both of these...because they are trained on same tokenizer(sentencepiece tokenizer).There might be minor changes but as a whole I hope it will not creat ebig mess."},{"metadata":{"trusted":true},"cell_type":"code","source":"config = XLNetConfig.from_pretrained('../input/xlnet-base-tf/xlnet-base-cased-config.json')\ntokenizer = XLNetTokenizer.from_pretrained('../input/xlnet-base-tf/xlnet-base-cased-spiece.model' ,do_lower_case = True)\nxlnet = TFXLNetModel.from_pretrained('../input/xlnet-base-tf/xlnet-base-cased-tf_model.h5',config=config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = AlbertConfig.from_pretrained('../input/albert-base-v2-tf2/config.json')\nalbert = TFAlbertModel.from_pretrained('../input/albert-base-v2-tf2/tf_model.h5',config=config)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### prints a random tokenized sentence"},{"metadata":{"trusted":true},"cell_type":"code","source":"k = random.randrange(train.shape[0])\nexample = train.loc[k ,'text']\nenc = tokenizer.encode(example)\nprint('statement is \\'{}\\''.format(example))\nprint('encoding is {}'.format(enc))\nsentence = ''\nfor en in enc:\n    token = tokenizer._convert_id_to_token(en)\n    print('{} : {}'.format(token ,en))\n    if token != '<sep>' and token != '<cls>':\n        sentence = sentence + token\nsentence = sentence.replace('‚ñÅ' ,\" \").strip()\nprint(sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.encode('<pad>')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 100\nspecial_token = {'<sep>':4 ,'<cls>':3 ,'<pad>':5}\npositive = tokenizer.encode('positive')[:-2]\nnegative = tokenizer.encode('negative')[:-2]\nneutral = tokenizer.encode('neutral')[:-2]\nsent_tokens  ={'positive': positive ,'negative':negative ,'neutral' : neutral}\nprint(sent_tokens['positive'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = train.shape[0]\ntoken_ids = np.zeros((ct ,MAX_LEN))\nattention_masks = np.zeros((ct ,MAX_LEN))\ntoken_type_ids = np.zeros((ct ,MAX_LEN))\nstart_token = np.zeros((ct ,MAX_LEN))\nend_token = np.zeros((ct ,MAX_LEN))\ncounter = 0\nfor k in range(ct):\n    try:\n        text = train.loc[k ,'text']\n        text = \" \" + \" \".join(text.split())\n        selected_text = train.loc[k ,'selected_text']\n        selected_text = \" \".join(selected_text.split())\n        sent = train.loc[k ,'sentiment']\n        text_enc = tokenizer.encode(text)[:-2]\n        selected_enc = tokenizer.encode(selected_text)[:-2]\n        idx = text.find(selected_text)\n        chars = np.zeros((len(text)))\n        chars[idx:idx+len(selected_text)] = 1\n        if text[idx-1] == ' ':chars[idx-1] =1\n    \n        offsets = []\n        idx = 0\n        for en in text_enc:\n            token = tokenizer._convert_id_to_token(en)\n            offsets.append((idx ,idx+len(token)))\n            idx += len(token)\n        toks = []\n        for i ,(a ,b) in enumerate(offsets):\n            if np.sum(chars[a : b]) >0 :\n                toks.append(i)\n        sp = special_token['<sep>']\n        cl = special_token['<cls>']\n        pad = special_token['<pad>']\n        enc_final = [cl] + text_enc + [sp ,sp] + sent_tokens[sent] + [sp]\n        token_ids[k ,:] = enc_final + (MAX_LEN - len(enc_final))*[0]\n        attention_masks[k ,:] = len(enc_final)*[1] + (MAX_LEN-len(enc_final))*[0]\n        token_type_ids[k ,len(enc_final)-2:len(enc_final)] = 1\n        start_token[k ,toks[0] + 1] = 1\n        end_token[k ,toks[-1] + 1] = 1 \n    except IndexError:\n        token_ids[k ,:] = 0\n        attention_masks[k ,:] = 0\n        token_type_ids[k ,:] = 0\n        start_token[k ,0] = 1\n        end_token[k ,0] = 1\n    if k == 2:\n        print(start_token[k])\n        print(end_token[k])\n        print(text)\n        print(token_ids[k])\n        print(selected_text)\n        a = np.argmax(start_token[k])\n        b = np.argmax(end_token[k])\n        man = tokenizer.encode(text)[:-2]\n        print(tokenizer.decode(man[a-1:b]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.loc[314 ,'text'])\nprint(token_ids[314])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna(' ')\nct_t = test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 100\ntoken_ids_t = np.zeros((ct_t ,MAX_LEN))\nattention_masks_t = np.zeros((ct_t ,MAX_LEN))\ntoken_type_ids_t = np.zeros((ct_t ,MAX_LEN))\nfor k in range(ct_t):\n    text = test.loc[k ,'text']\n    text = \" \" + \" \".join(text.split())\n    sent = test.loc[k ,'sentiment']\n    text_enc = tokenizer.encode(text)[:-2]\n    sp = special_token['<sep>']\n    cl = special_token['<cls>']\n    pad = special_token['<pad>']\n    enc_final = [cl] + text_enc + [sp ,sp] + sent_tokens[sent] + [sp] \n    token_ids_t[k ,:] = enc_final + (MAX_LEN - len(enc_final))*[0]\n    attention_masks_t[k ,:] = len(enc_final)*[1] + (MAX_LEN-len(enc_final))*[0]\n    token_type_ids_t[k ,len(enc_final)-3:len(enc_final)] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(mod):\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    x = mod(ids,attention_mask=att,token_type_ids=tok)\n\n    drop1 = tf.keras.layers.Dropout(0.1)(x[0])\n    #layer1 = tf.keras.layers.Conv1D(128 ,kernel_size = 2 ,padding = 'same')(drop1)\n    #layer1 = tf.keras.layers.LeakyReLU()(layer1)\n    layer2 = tf.keras.layers.Conv1D(64 ,kernel_size = 1)(drop1)\n    layer2 = tf.keras.layers.LeakyReLU()(layer2)\n    layer3 = tf.keras.layers.Dense(1)(layer2)\n    layer4 = tf.keras.layers.Flatten()(layer3)\n    output_1 = tf.keras.layers.Activation('softmax')(layer4)\n\n    drop1_ = tf.keras.layers.Dropout(0.2)(x[0])\n    #layer1_ = tf.keras.layers.Conv1D(128 ,kernel_size = 2 ,padding = 'same')(drop1_)\n    #layer1_ = tf.keras.layers.LeakyReLU()(layer1_)\n    layer2_ = tf.keras.layers.Conv1D(64 ,kernel_size = 1 )(drop1_)\n    layer2_ = tf.keras.layers.LeakyReLU()(layer2_)\n    layer3_ = tf.keras.layers.Dense(1)(layer2_)\n    layer4_ = tf.keras.layers.Flatten()(layer3_)\n    output_2 = tf.keras.layers.Activation('softmax')(layer4_)\n    model = tf.keras.Model(inputs = [ids ,att ,tok] ,outputs = [output_1 ,output_2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.000001)\n    model.compile(loss = my_loss(1.5) ,optimizer = optimizer)\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining loss and metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_loss(gamma):\n    '''defining focal loss with gamma and alpha parameters'''\n    def focal_loss(y_true ,y_pred):\n        y_true = tf.cast(y_true ,dtype = tf.float32)\n        y_pred = tf.cast(y_pred ,dtype = tf.float32)\n        log_lik = ((1-y_pred)**gamma)*y_true*K.log(y_pred) + (y_pred**gamma)*(1-y_true)*K.log(1-y_pred)\n        return -K.sum(log_lik ,axis = -1)\n    return focal_loss\ndef category(y_true ,y_pred):\n    y_pred = tf.keras.backend.clip(y_pred ,1e-7 ,1-1e-7)\n    return tf.keras.losses.binary_crossentropy(y_true ,y_pred)\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\ndef scheduler(epoch):\n    return 3e-5 * 0.2**epoch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training xlnet"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = build_model(xlnet)\nmodel1.compile(optimizer = Adam(lr = 3e-5) ,loss = my_loss(1.5))\nreduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\nhist = model1.fit([token_ids[800: ,], attention_masks[800: ,], token_type_ids[800:,]], [start_token[800:,], end_token[800:,]], \n        epochs=3, batch_size=8, verbose=1, callbacks=[reduce_lr],\n        validation_split = 0.1)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''model1.save_weights('xlnet.h5')'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training alBERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = build_model(albert)\nmodel2.compile(optimizer = Adam(lr = 3e-5) ,loss = my_loss(1.5))\nreduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\nhist = model2.fit([token_ids[800: ,], attention_masks[800: ,], token_type_ids[800:,]], [start_token[800:,], end_token[800:,]], \n        epochs=6, batch_size=8, verbose=1, callbacks=[reduce_lr],\n        validation_split = 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model2.save_weights('albert.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model1.load_weights('xlnet.h5')\n#model2.load_weights('albert.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XLNet predections"},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_start1 = np.zeros((token_ids.shape[0],MAX_LEN))\noof_end1 = np.zeros((token_ids.shape[0],MAX_LEN))\noof_start1,oof_end1 = model1.predict([token_ids[:800],attention_masks[:800],token_type_ids[:800]],verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## alBERT predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_start2 = np.zeros((token_ids.shape[0],MAX_LEN))\noof_end2 = np.zeros((token_ids.shape[0],MAX_LEN))\noof_start2,oof_end2 = model2.predict([token_ids[:800],attention_masks[:800],token_type_ids[:800]],verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Combining predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_start = np.zeros((token_ids.shape[0],MAX_LEN))\noof_end = np.zeros((token_ids.shape[0],MAX_LEN))\noof_start,oof_end = (oof_start1+oof_start2)/2 ,(oof_end1+oof_end2)/2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculating jaccard"},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\njac = []\ncounter = 0\nm = random.randrange(800)\nfor k in range(800):\n    a = np.argmax(oof_start[k,])\n    b = np.argmax(oof_end[k,])\n    if a>b: \n        st = train.loc[k,'text']\n        # IMPROVE CV/LB with better choice here\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    else:\n        text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)[:-2]\n        st = tokenizer.decode(enc[a-1:b])\n        if k==8:\n            print(oof_start[k ,])\n            print(oof_end[k ,])\n            print(a ,b)\n            print(st)\n            print(text1)\n            print(train.loc[k ,'selected_text'])\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    \njac.append(np.mean(all))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(jac)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Kaggle submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds1 = model1.predict([token_ids_t ,attention_masks_t ,token_type_ids_t] ,verbose = 1)\npreds2 = model2.predict([token_ids_t ,attention_masks_t ,token_type_ids_t] ,verbose = 1)\npreds_start = (preds1[0] + preds2[0])/2\npreds_end = (preds1[1] + preds2[1])/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\nfor k in range(token_ids_t.shape[0]):\n    a = np.argmax([preds_start[k ,]])\n    b = np.argmax(preds_end[k ,])\n    if a>b:\n        st = test.loc[k ,'text']\n    else:\n        text1 = \" \" + \" \".join(test.loc[k ,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc[a-1:b])\n    all.append(st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['selected_text'] = all\ntest[['textID' ,'selected_text']].to_csv('submission.csv' ,index = False)\npd.set_option('max_colwidth' ,60)\ntest.sample(25)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}