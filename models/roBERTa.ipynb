{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 96\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file='../input/tf-roberta/vocab-roberta-base.json', \n",
    "    merges_file='../input/tf-roberta/merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "EPOCHS = 3 # originally 3\n",
    "BATCH_SIZE = 32 # originally 32\n",
    "PAD_ID = 1\n",
    "#SEED = 88888\n",
    "LABEL_SMOOTHING = 0.1\n",
    "#tf.random.set_seed(SEED)\n",
    "#np.random.seed(SEED)\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "train.head()\n",
    "def remove_html(text):\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    return text\n",
    "def scheduler(epoch):\n",
    "    return 3e-5 * 0.2**epoch\n",
    "reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " my boss is bullying me...\n",
      "[    0  2430   127  3504    16 11902   162   734     2     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1]\n",
      "bullying me\n",
      " bullying me\n"
     ]
    }
   ],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "    attention_mask[k,:len(enc.ids)+3] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+2] = 1\n",
    "        end_tokens[k,toks[-1]+2] = 1\n",
    "    if k == 2:\n",
    "        print(start_tokens[k])\n",
    "        print(end_tokens[k])\n",
    "        print(text1)\n",
    "        print(input_ids[k])\n",
    "        print(text2)\n",
    "        a = np.argmax(start_tokens[k])\n",
    "        b = np.argmax(end_tokens[k])\n",
    "        man = tokenizer.encode(text1)\n",
    "        print(tokenizer.decode(enc.ids[a-2:b-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model, weight_fn):\n",
    "    with open(weight_fn, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "    model.set_weights(weights)\n",
    "    return model\n",
    "def save_weights(model, dst_fn):\n",
    "    weights = model.get_weights()\n",
    "    with open(dst_fn, 'wb') as f:\n",
    "        pickle.dump(weights, f)\n",
    "def loss_fn(y_true, y_pred):\n",
    "    # adjust the targets for sequence bucketing\n",
    "    ll = tf.shape(y_pred)[1]\n",
    "    y_true = y_true[:, :ll]\n",
    "    batch_size = tf.shape(y_true)[0]\n",
    "    LEN = ll \n",
    "    ind_row = tf.range(0, LEN)\n",
    "    ones_matrix = tf.ones([batch_size, LEN])\n",
    "    #print(K.int_shape(ones_matrix))\n",
    "    ind_row = tf.cast(ind_row ,dtype = tf.float32)\n",
    "    ones_matrix = tf.cast(ones_matrix ,dtype = tf.float32)     \n",
    "    ind_matrix = ind_row * ones_matrix\n",
    "    k1 = tf.cast(tf.math.argmax(y_true ,axis = 1) ,dtype = tf.float32)\n",
    "    k2 = K.sum(y_pred * ind_matrix, axis=1)\n",
    "    y_pred = tf.cast(y_pred ,dtype = tf.float32) \n",
    "    y_true = tf.cast(y_true ,dtype = tf.float32) \n",
    "    loss1 = tf.keras.losses.binary_crossentropy(y_true, y_pred,\n",
    "        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n",
    "    loss2 = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n",
    "        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n",
    "    loss = tf.reduce_mean((loss1+loss2))\n",
    "    return loss\n",
    "def build_model():\n",
    "    config = RobertaConfig.from_pretrained('../input/tf-roberta/config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained('../input/tf-roberta/pretrained-roberta-base.h5',config=config)\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
    "\n",
    "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
    "    max_len = tf.reduce_max(lens)\n",
    "    ids_ = ids[:, :max_len]\n",
    "    att_ = att[:, :max_len]\n",
    "    tok_ = tok[:, :max_len]\n",
    "    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.15)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(1536, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.15)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(1536, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(128, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n",
    "    model.compile(loss=loss_fn ,optimizer=optimizer)\n",
    "    \n",
    "    # this is required as `model.predict` needs a fixed size!\n",
    "    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "    \n",
    "    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n",
    "    return model, padded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "21984/21984 [==============================] - 134s 6ms/sample - loss: 3.4501 - activation_loss: 1.7335 - activation_1_loss: 1.7165 - val_loss: 3.0940 - val_activation_loss: 1.5593 - val_activation_1_loss: 1.5368\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 2/2\n",
      "21984/21984 [==============================] - 108s 5ms/sample - loss: 3.0540 - activation_loss: 1.5463 - activation_1_loss: 1.5077 - val_loss: 3.0610 - val_activation_loss: 1.5464 - val_activation_1_loss: 1.5168\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 3/3\n",
      "21984/21984 [==============================] - 107s 5ms/sample - loss: 2.9873 - activation_loss: 1.5144 - activation_1_loss: 1.4729 - val_loss: 3.0565 - val_activation_loss: 1.5449 - val_activation_1_loss: 1.5139\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5497/5497 [==============================] - 16s 3ms/sample\n",
      ">>>> FOLD 1 Jaccard = 0.71475117714835\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 9s 2ms/sample\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "21985/21985 [==============================] - 138s 6ms/sample - loss: 3.5608 - activation_loss: 1.7924 - activation_1_loss: 1.7666 - val_loss: 3.1915 - val_activation_loss: 1.6243 - val_activation_1_loss: 1.5688\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 2/2\n",
      "21985/21985 [==============================] - 115s 5ms/sample - loss: 3.1243 - activation_loss: 1.5835 - activation_1_loss: 1.5387 - val_loss: 3.0991 - val_activation_loss: 1.5731 - val_activation_1_loss: 1.5278\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 3/3\n",
      "21985/21985 [==============================] - 116s 5ms/sample - loss: 3.0647 - activation_loss: 1.5574 - activation_1_loss: 1.5135 - val_loss: 3.0797 - val_activation_loss: 1.5617 - val_activation_1_loss: 1.5199\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 16s 3ms/sample\n",
      ">>>> FOLD 2 Jaccard = 0.706172925958753\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 9s 2ms/sample\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "21985/21985 [==============================] - 132s 6ms/sample - loss: 3.5183 - activation_loss: 1.7654 - activation_1_loss: 1.7526 - val_loss: 3.1384 - val_activation_loss: 1.5901 - val_activation_1_loss: 1.5496\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 2/2\n",
      " 2592/21985 [==>...........................] - ETA: 1:27 - loss: 3.2182 - activation_loss: 1.6155 - activation_1_loss: 1.6027"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN) )\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN) )\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN) )\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN) )\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True)#,random_state=SEED) #originally 5 splits\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model, padded_model = build_model()\n",
    "        \n",
    "    #sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "    #    '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "    #    save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n",
    "    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n",
    "    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n",
    "    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n",
    "    # sort the validation data\n",
    "    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n",
    "    inpV = [arr[shuffleV] for arr in inpV]\n",
    "    targetV = [arr[shuffleV] for arr in targetV]\n",
    "    weight_fn = '3-%s-roberta-%i.h5'%(VER,fold)\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # sort and shuffle: We add random numbers to not have the same order in each epoch\n",
    "        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n",
    "        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n",
    "        num_batches = math.ceil(len(shuffleT) / BATCH_SIZE)\n",
    "        batch_inds = np.random.permutation(num_batches)\n",
    "        shuffleT_ = []\n",
    "        for batch_ind in batch_inds:\n",
    "            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n",
    "        shuffleT = np.concatenate(shuffleT_)\n",
    "        # reorder the input data\n",
    "        inpT = [arr[shuffleT] for arr in inpT]\n",
    "        targetT = [arr[shuffleT] for arr in targetT]\n",
    "        model.fit(inpT, targetT, \n",
    "            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY,\n",
    "            validation_data=(inpV, targetV), shuffle=False ,callbacks = [reduce_lr])  # don't shuffle in `fit`\n",
    "        save_weights(model, weight_fn)\n",
    "\n",
    "    print('Loading model...')\n",
    "    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    load_weights(model, weight_fn)\n",
    "\n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-2:b-1])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print('Predicting Test...')\n",
    "    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i am very much delighted\n"
     ]
    }
   ],
   "source": [
    "test_input_id = np.ones((1 ,MAX_LEN))\n",
    "test_attention_mask = np.zeros((1 ,MAX_LEN))\n",
    "test_token_type_id = np.zeros((1 ,MAX_LEN))\n",
    "sentence =  'I am very much delighted'\n",
    "sent = 'positive'\n",
    "sentence = ' '+' '.join(sentence.split())\n",
    "enc = tokenizer.encode(sentence)              \n",
    "s_tok = sentiment_id[sent]\n",
    "test_input_id[0 ,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "attention_mask_t[0,:len(enc.ids)+3] = 1\n",
    "start ,end = padded_model.predict([test_input_id ,test_attention_mask ,test_token_type_id])\n",
    "a = np.argmax(start[0 ,])\n",
    "b = np.argmax(end[0 ,])\n",
    "if a>b:\n",
    "    print(sentence)\n",
    "else:\n",
    "    selected = tokenizer.decode(enc.ids[a-2:b-1])\n",
    "    print(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-2:b-1])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>a2867122b8</td>\n",
       "      <td>This morning I rode behind a guy with a bird cage contai...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i couldn`t get a photo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>d79f6eda9d</td>\n",
       "      <td>NOPE.  It`s been years since it`s happened like this.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>nope. it`s been years since it`s happened like this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>68a9b81741</td>\n",
       "      <td>Finally got to wash my hair! I feel much better now, but...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i feel much better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>8e3b60deaa</td>\n",
       "      <td>No problem. At least look on the floor. We won`t see th...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>no problem. at least look on the floor. we won`t see th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>105b761a2c</td>\n",
       "      <td>i dont even know now lenaaa  when you going clothes show?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i dont even know now lenaaa when you going clothes show?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>1fb696a58c</td>\n",
       "      <td>We just hit 10000 views on myspace! Thanks everyone!</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>d24c474353</td>\n",
       "      <td>I know it`s against the law... I am a rebel in small th...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i know it`s against the law... i am a rebel in small th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>bd3b30d13c</td>\n",
       "      <td>Hey, hey, happy mother`s day!  http://plurk.com/p/svwii</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1857</th>\n",
       "      <td>2f8250d0df</td>\n",
       "      <td>the day goes on and on...i think im gonna write a song a...</td>\n",
       "      <td>negative</td>\n",
       "      <td>impossible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>536116702e</td>\n",
       "      <td>hmmm dollhouse sounds pretty good, http://bit.ly/z3aZv  ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>good,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2408</th>\n",
       "      <td>63a1fa4db7</td>\n",
       "      <td>thanks for the answer  I had been wondering the whole week</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3354</th>\n",
       "      <td>c25f1544bd</td>\n",
       "      <td>There is a MacBook Pro sitting close by and my poor MacB...</td>\n",
       "      <td>negative</td>\n",
       "      <td>ashamed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>384a8c1c53</td>\n",
       "      <td>It depends on your goals &amp; how much you want to spend  ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>7880cbca66</td>\n",
       "      <td>Darn, I thought you meant White Sox.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>darn, i thought you meant white sox.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>f27f60c847</td>\n",
       "      <td>Yep. Nothing to worry about.</td>\n",
       "      <td>positive</td>\n",
       "      <td>nothing to worry about.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3352</th>\n",
       "      <td>3cdfcc7a2c</td>\n",
       "      <td>It`s not a reg gig yet, but hopefully it will be! Haven...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>it`s not a reg gig yet, but hopefully it will be! haven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>aa221b6a7d</td>\n",
       "      <td>hey, what about us followers in ATL!!!!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hey, what about us followers in atl!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>8429087609</td>\n",
       "      <td>Wango tango!!! Good night all</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wango tango!!! good night all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>b78e0c4c19</td>\n",
       "      <td>Thank you Kirst - your posts on running inspired me</td>\n",
       "      <td>positive</td>\n",
       "      <td>thank you kirst - your posts on running inspired me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3251</th>\n",
       "      <td>e37a7b9128</td>\n",
       "      <td>Ouch, give me a heads up so I`ll know when to duck</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ouch, give me a heads up so i`ll know when to duck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2866</th>\n",
       "      <td>fd5ee5b52e</td>\n",
       "      <td>happy mothers day mommy  i love you so much, dono what i...</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>44b0b41b62</td>\n",
       "      <td>BabyLove em homenagem ao Baby D</td>\n",
       "      <td>positive</td>\n",
       "      <td>babylove</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3267</th>\n",
       "      <td>49e25011f6</td>\n",
       "      <td>_HK very funny</td>\n",
       "      <td>positive</td>\n",
       "      <td>very funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>a11da2d3bf</td>\n",
       "      <td>Playin City of Villains, wishin my buddies were playin w...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wishin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2921</th>\n",
       "      <td>04c455e820</td>\n",
       "      <td>Does `Real Detroit Weekly` not have a website.....Oh the...</td>\n",
       "      <td>negative</td>\n",
       "      <td>oh the horror, the horror</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "724   a2867122b8  This morning I rode behind a guy with a bird cage contai...   \n",
       "2828  d79f6eda9d        NOPE.  It`s been years since it`s happened like this.   \n",
       "954   68a9b81741  Finally got to wash my hair! I feel much better now, but...   \n",
       "2144  8e3b60deaa   No problem. At least look on the floor. We won`t see th...   \n",
       "836   105b761a2c    i dont even know now lenaaa  when you going clothes show?   \n",
       "1385  1fb696a58c         We just hit 10000 views on myspace! Thanks everyone!   \n",
       "2129  d24c474353   I know it`s against the law... I am a rebel in small th...   \n",
       "1373  bd3b30d13c      Hey, hey, happy mother`s day!  http://plurk.com/p/svwii   \n",
       "1857  2f8250d0df  the day goes on and on...i think im gonna write a song a...   \n",
       "1648  536116702e  hmmm dollhouse sounds pretty good, http://bit.ly/z3aZv  ...   \n",
       "2408  63a1fa4db7   thanks for the answer  I had been wondering the whole week   \n",
       "3354  c25f1544bd  There is a MacBook Pro sitting close by and my poor MacB...   \n",
       "694   384a8c1c53   It depends on your goals & how much you want to spend  ...   \n",
       "1292  7880cbca66                         Darn, I thought you meant White Sox.   \n",
       "839   f27f60c847                                 Yep. Nothing to worry about.   \n",
       "3352  3cdfcc7a2c   It`s not a reg gig yet, but hopefully it will be! Haven...   \n",
       "1220  aa221b6a7d                      hey, what about us followers in ATL!!!!   \n",
       "815   8429087609                                Wango tango!!! Good night all   \n",
       "1455  b78e0c4c19          Thank you Kirst - your posts on running inspired me   \n",
       "3251  e37a7b9128           Ouch, give me a heads up so I`ll know when to duck   \n",
       "2866  fd5ee5b52e  happy mothers day mommy  i love you so much, dono what i...   \n",
       "2131  44b0b41b62                              BabyLove em homenagem ao Baby D   \n",
       "3267  49e25011f6                                               _HK very funny   \n",
       "120   a11da2d3bf  Playin City of Villains, wishin my buddies were playin w...   \n",
       "2921  04c455e820  Does `Real Detroit Weekly` not have a website.....Oh the...   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "724   negative                                      i couldn`t get a photo.  \n",
       "2828   neutral         nope. it`s been years since it`s happened like this.  \n",
       "954   positive                                           i feel much better  \n",
       "2144   neutral   no problem. at least look on the floor. we won`t see th...  \n",
       "836    neutral     i dont even know now lenaaa when you going clothes show?  \n",
       "1385  positive                                                       thanks  \n",
       "2129  positive   i know it`s against the law... i am a rebel in small th...  \n",
       "1373  positive                                                        happy  \n",
       "1857  negative                                                   impossible  \n",
       "1648  positive                                                        good,  \n",
       "2408  positive                                                       thanks  \n",
       "3354  negative                                                      ashamed  \n",
       "694   positive                                                         good  \n",
       "1292   neutral                         darn, i thought you meant white sox.  \n",
       "839   positive                                      nothing to worry about.  \n",
       "3352   neutral   it`s not a reg gig yet, but hopefully it will be! haven...  \n",
       "1220   neutral                      hey, what about us followers in atl!!!!  \n",
       "815    neutral                                wango tango!!! good night all  \n",
       "1455  positive          thank you kirst - your posts on running inspired me  \n",
       "3251   neutral           ouch, give me a heads up so i`ll know when to duck  \n",
       "2866  positive                                                        happy  \n",
       "2131  positive                                                     babylove  \n",
       "3267  positive                                                   very funny  \n",
       "120   positive                                                       wishin  \n",
       "2921  negative                                    oh the horror, the horror  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
