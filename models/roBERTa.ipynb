{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "import pickle\n",
    "\n",
    "\n",
    "import math\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 96\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file='../input/tf-roberta/vocab-roberta-base.json', \n",
    "    merges_file='../input/tf-roberta/merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "EPOCHS = 3 # originally 3\n",
    "BATCH_SIZE = 32 # originally 32\n",
    "PAD_ID = 1\n",
    "#SEED = 88888\n",
    "LABEL_SMOOTHING = 0.1\n",
    "#tf.random.set_seed(SEED)\n",
    "#np.random.seed(SEED)\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('../input//tweet-sentiment-extraction/train.csv').fillna('')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " my boss is bullying me...\n",
      "[    0  2430     2     2   127  3504    16 11902   162   734     2     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1]\n",
      "bullying me\n",
      " bullying me\n"
     ]
    }
   ],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "input_ids_ = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_ = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "sentiments =  np.zeros((ct ,3))\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "    if train.loc[k ,'sentiment'] == 'positive':\n",
    "        sentiments[k][0] = 1\n",
    "    elif train.loc[k ,'sentiment'] == 'negative':\n",
    "        sentiments[k][1] = 1\n",
    "    elif train.loc[k ,'sentiment'] == 'neutral':\n",
    "        sentiments[k][2] = 1\n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0, s_tok ,2 ,2] + enc.ids + [2]\n",
    "    input_ids_[k,:len(enc.ids)+2] = [0] + enc.ids + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    attention_mask_[k,:len(enc.ids)+2] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+4] = 1\n",
    "        end_tokens[k,toks[-1]+4] = 1\n",
    "    if k == 2:\n",
    "        print(start_tokens[k])\n",
    "        print(end_tokens[k])\n",
    "        print(text1)\n",
    "        print(input_ids[k])\n",
    "        print(text2)\n",
    "        a = np.argmax(start_tokens[k])\n",
    "        b = np.argmax(end_tokens[k])\n",
    "        man = tokenizer.encode(text1)\n",
    "        print(tokenizer.decode(enc.ids[a-4:b-3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "input_ids_t_ = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t_ = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0, s_tok ,2 ,2] + enc.ids + [2]\n",
    "    input_ids_t_[k,:len(enc.ids)+2] = [0] + enc.ids + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1\n",
    "    attention_mask_t_[k,:len(enc.ids)+2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig.from_pretrained('../input/tf-roberta/config-roberta-base.json')\n",
    "bert_model = TFRobertaModel.from_pretrained('../input/tf-roberta/pretrained-roberta-base.h5',config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(model, dst_fn):\n",
    "    weights = model.get_weights()\n",
    "    with open(dst_fn, 'wb') as f:\n",
    "        pickle.dump(weights, f)\n",
    "\n",
    "\n",
    "def load_weights(model, weight_fn):\n",
    "    with open(weight_fn, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "    model.set_weights(weights)\n",
    "    return model\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    # adjust the targets for sequence bucketing\n",
    "    ll = tf.shape(y_pred)[1]\n",
    "    y_true = y_true[:, :ll]\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n",
    "        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "def scheduler(epoch):\n",
    "    return 3e-5 * 0.2**epoch\n",
    "\n",
    "def build_model():\n",
    "    config = RobertaConfig.from_pretrained('../input/tf-roberta/config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained('../input/tf-roberta/pretrained-roberta-base.h5',config=config)\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    ids1 = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att1 = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
    "    \n",
    "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
    "    max_len = tf.reduce_max(lens)\n",
    "    ids_ = ids[:, :max_len]\n",
    "    att_ = att[:, :max_len]\n",
    "    tok_ = tok[:, :max_len]\n",
    "    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n",
    "    x_ = bert_model(ids1,attention_mask=att1)\n",
    "    x1 = tf.keras.layers.Dropout(0.4)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(1024, 2,padding='same')(x1)\n",
    "    x1_ = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dropout(0.4)(x1_)\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.4)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(1024, 2,padding='same')(x2)\n",
    "    x2_ = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Dropout(0.4)(x2_)\n",
    "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "    x_ = tf.keras.layers.Conv1D(1024, 2,padding='same')(x_[0])\n",
    "    x_ = tf.keras.layers.Dense(1)(x_)\n",
    "    x_ = tf.keras.layers.LeakyReLU()(x_)\n",
    "    x_ = tf.keras.layers.Flatten()(x_)\n",
    "    x_ = tf.keras.layers.Dense(3 ,activation = 'softmax')(x_)\n",
    "    model = tf.keras.models.Model(inputs=[ids,ids1 ,att ,att1 ,tok], outputs=[x1,x2 ,x_])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n",
    "    model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "    \n",
    "    # this is required as `model.predict` needs a fixed size!\n",
    "    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "    padded_model = tf.keras.models.Model(inputs=[ids , att, tok], outputs=[x1_padded,x2_padded])\n",
    "    return model, padded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "21984/21984 [==============================] - 413s 19ms/sample - loss: 3.9535 - activation_loss: 1.5709 - activation_1_loss: 1.5711 - dense_3_loss: 0.8114 - val_loss: 4.2578 - val_activation_loss: 1.2975 - val_activation_1_loss: 1.2715 - val_dense_3_loss: 1.6892\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 2/2\n",
      "21984/21984 [==============================] - 371s 17ms/sample - loss: 3.2467 - activation_loss: 1.3261 - activation_1_loss: 1.2925 - dense_3_loss: 0.6280 - val_loss: 4.4185 - val_activation_loss: 1.2790 - val_activation_1_loss: 1.2529 - val_dense_3_loss: 1.8871\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 3/3\n",
      "21984/21984 [==============================] - 371s 17ms/sample - loss: 3.1572 - activation_loss: 1.2926 - activation_1_loss: 1.2599 - dense_3_loss: 0.6048 - val_loss: 4.4770 - val_activation_loss: 1.2744 - val_activation_1_loss: 1.2500 - val_dense_3_loss: 1.9530\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5497/5497 [==============================] - 17s 3ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 9s 3ms/sample\n",
      ">>>> FOLD 1 Jaccard = 0.710678326853381\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "21985/21985 [==============================] - 426s 19ms/sample - loss: 3.9799 - activation_loss: 1.5824 - activation_1_loss: 1.5980 - dense_3_loss: 0.7963 - val_loss: 4.2962 - val_activation_loss: 1.3444 - val_activation_1_loss: 1.3078 - val_dense_3_loss: 1.6458\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 2/2\n",
      "21985/21985 [==============================] - 378s 17ms/sample - loss: 3.2774 - activation_loss: 1.3304 - activation_1_loss: 1.3184 - dense_3_loss: 0.6272 - val_loss: 4.4063 - val_activation_loss: 1.2913 - val_activation_1_loss: 1.2776 - val_dense_3_loss: 1.8393\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 3/3\n",
      "21985/21985 [==============================] - 377s 17ms/sample - loss: 3.1855 - activation_loss: 1.2993 - activation_1_loss: 1.2812 - dense_3_loss: 0.6056 - val_loss: 4.4208 - val_activation_loss: 1.2947 - val_activation_1_loss: 1.2766 - val_dense_3_loss: 1.8513\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 18s 3ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 9s 3ms/sample\n",
      ">>>> FOLD 2 Jaccard = 0.7089116898957064\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "21985/21985 [==============================] - 426s 19ms/sample - loss: 3.9997 - activation_loss: 1.5701 - activation_1_loss: 1.5717 - dense_3_loss: 0.8550 - val_loss: 4.3766 - val_activation_loss: 1.3167 - val_activation_1_loss: 1.3281 - val_dense_3_loss: 1.7344\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 2/2\n",
      "21985/21985 [==============================] - 375s 17ms/sample - loss: 3.3137 - activation_loss: 1.3477 - activation_1_loss: 1.3136 - dense_3_loss: 0.6550 - val_loss: 4.4013 - val_activation_loss: 1.2864 - val_activation_1_loss: 1.2341 - val_dense_3_loss: 1.8833\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 3/3\n",
      "21985/21985 [==============================] - 370s 17ms/sample - loss: 3.1981 - activation_loss: 1.3029 - activation_1_loss: 1.2672 - dense_3_loss: 0.6268 - val_loss: 4.3611 - val_activation_loss: 1.2794 - val_activation_1_loss: 1.2341 - val_dense_3_loss: 1.8500\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 18s 3ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 9s 2ms/sample\n",
      ">>>> FOLD 3 Jaccard = 0.7099556268441733\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "21985/21985 [==============================] - 421s 19ms/sample - loss: 4.1013 - activation_loss: 1.6079 - activation_1_loss: 1.6147 - dense_3_loss: 0.8804 - val_loss: 4.2321 - val_activation_loss: 1.3209 - val_activation_1_loss: 1.2898 - val_dense_3_loss: 1.6217\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 2/2\n",
      "21985/21985 [==============================] - 373s 17ms/sample - loss: 3.2862 - activation_loss: 1.3329 - activation_1_loss: 1.3022 - dense_3_loss: 0.6513 - val_loss: 4.4015 - val_activation_loss: 1.2776 - val_activation_1_loss: 1.2398 - val_dense_3_loss: 1.8839\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 3/3\n",
      "21985/21985 [==============================] - 370s 17ms/sample - loss: 3.1824 - activation_loss: 1.2922 - activation_1_loss: 1.2649 - dense_3_loss: 0.6248 - val_loss: 4.4387 - val_activation_loss: 1.2739 - val_activation_1_loss: 1.2364 - val_dense_3_loss: 1.9282\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 17s 3ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 9s 3ms/sample\n",
      ">>>> FOLD 4 Jaccard = 0.7075765109922423\n",
      "\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "21985/21985 [==============================] - 420s 19ms/sample - loss: 4.0864 - activation_loss: 1.6127 - activation_1_loss: 1.5867 - dense_3_loss: 0.8852 - val_loss: 4.2791 - val_activation_loss: 1.2988 - val_activation_1_loss: 1.2662 - val_dense_3_loss: 1.7156\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 2/2\n",
      "21985/21985 [==============================] - 378s 17ms/sample - loss: 3.3098 - activation_loss: 1.3474 - activation_1_loss: 1.3067 - dense_3_loss: 0.6546 - val_loss: 4.2977 - val_activation_loss: 1.2857 - val_activation_1_loss: 1.2415 - val_dense_3_loss: 1.7720\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 3/3\n",
      "21985/21985 [==============================] - 373s 17ms/sample - loss: 3.1857 - activation_loss: 1.3020 - activation_1_loss: 1.2628 - dense_3_loss: 0.6224 - val_loss: 4.3950 - val_activation_loss: 1.2778 - val_activation_1_loss: 1.2373 - val_dense_3_loss: 1.8813\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 17s 3ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 9s 2ms/sample\n",
      ">>>> FOLD 5 Jaccard = 0.712410907166362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True) #,random_state=SEED) #originally 5 splits\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model, padded_model = build_model()\n",
    "        \n",
    "    #sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "    #    '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "    #    save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "    inpT = [input_ids[idxT,],input_ids_[idxT,] ,attention_mask[idxT,],attention_mask_[idxT,], token_type_ids[idxT,]]\n",
    "    targetT = [start_tokens[idxT,], end_tokens[idxT,] ,sentiments[idxT ,]]\n",
    "    inpV = [input_ids[idxV,],input_ids_[idxV,],attention_mask[idxV,],attention_mask_[idxV,] ,token_type_ids[idxV,]]\n",
    "    targetV = [start_tokens[idxV,], end_tokens[idxV,] ,sentiments[idxT ,]]\n",
    "    # sort the validation data\n",
    "    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n",
    "    inpV = [arr[shuffleV] for arr in inpV]\n",
    "    targetV = [arr[shuffleV] for arr in targetV]\n",
    "    weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # sort and shuffle: We add random numbers to not have the same order in each epoch\n",
    "        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n",
    "        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n",
    "        num_batches = math.ceil(len(shuffleT) / BATCH_SIZE)\n",
    "        batch_inds = np.random.permutation(num_batches)\n",
    "        shuffleT_ = []\n",
    "        for batch_ind in batch_inds:\n",
    "            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n",
    "        shuffleT = np.concatenate(shuffleT_)\n",
    "        # reorder the input data\n",
    "        inpT = [arr[shuffleT] for arr in inpT]\n",
    "        targetT = [arr[shuffleT] for arr in targetT]\n",
    "        reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "        model.fit(inpT, targetT, \n",
    "            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[reduce_lr],\n",
    "            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n",
    "        save_weights(model, weight_fn)\n",
    "\n",
    "    print('Loading model...')\n",
    "    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    load_weights(model, weight_fn)\n",
    "\n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-4:b-3])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> OVERALL 5Fold CV Jaccard = 0.709906612350373\n"
     ]
    }
   ],
   "source": [
    "print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.710678326853381, 0.7089116898957064, 0.7099556268441733, 0.7075765109922423, 0.712410907166362]\n"
     ]
    }
   ],
   "source": [
    "print(jac) # Jaccard CVs\n",
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-2:b-1])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>b09213d54b</td>\n",
       "      <td>no. my school will start on June1.  two days to go. i s...</td>\n",
       "      <td>negative</td>\n",
       "      <td>don`t want. :l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063</th>\n",
       "      <td>9f23e5d8f5</td>\n",
       "      <td>someone doesn`t feel good...</td>\n",
       "      <td>negative</td>\n",
       "      <td>`t feel good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>5ff525b74b</td>\n",
       "      <td>Wondering if i cld make things any worse than they alrea...</td>\n",
       "      <td>negative</td>\n",
       "      <td>they</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3372</th>\n",
       "      <td>817b6da55c</td>\n",
       "      <td>Okay, so the only reason I`m not buying this app is beca...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>so the only reason i`m not buying this app is because i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2777</th>\n",
       "      <td>3593dbe193</td>\n",
       "      <td>is all alone for the evening!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>alone for the evening!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>b56108fc36</td>\n",
       "      <td>hahaha niene ur soo smart.! lmao i think that is who it...</td>\n",
       "      <td>positive</td>\n",
       "      <td>in.. whts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>f61ae5403a</td>\n",
       "      <td>It`s 12AM! ,  and I are still up, sending you as much t...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>s 12am! , and i are still up, sending you as much tweets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>1b4612b286</td>\n",
       "      <td>I have a job at camp!!  Only downfall? No midnight showi...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a job at camp!! only downfall? no midnight showing of h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2846</th>\n",
       "      <td>44da94a9b7</td>\n",
       "      <td>Want to hang out with beth and tenaya and their hubbys t...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hang out with beth and tenaya and their hubbys tonight....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2905</th>\n",
       "      <td>416e6352b9</td>\n",
       "      <td>are you in need of another kiss attack??</td>\n",
       "      <td>neutral</td>\n",
       "      <td>in need of another kiss attack??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>7811012154</td>\n",
       "      <td>Not made it to work  couldn`t get up feelin blurgh</td>\n",
       "      <td>negative</td>\n",
       "      <td>t get up feelin blurgh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>0b25a31b7e</td>\n",
       "      <td>is surfin`..</td>\n",
       "      <td>neutral</td>\n",
       "      <td>in`..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>79256fbe62</td>\n",
       "      <td>Still no Internet today</td>\n",
       "      <td>neutral</td>\n",
       "      <td>internet today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>706a860942</td>\n",
       "      <td>_tron oh god, whose tire?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ron oh god, whose tire?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>0b3588bf63</td>\n",
       "      <td>Happy Mother`s Day all of the mom`s around the world!  I...</td>\n",
       "      <td>positive</td>\n",
       "      <td>`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>dec8c3dac3</td>\n",
       "      <td>HAPPY MOTHERS DAY MAMMA</td>\n",
       "      <td>positive</td>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>527b27665d</td>\n",
       "      <td>Just Chilling after MCFLY last night! ABSOULUTELY INCRED...</td>\n",
       "      <td>positive</td>\n",
       "      <td>utely incredible! =d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>5f91766356</td>\n",
       "      <td>aww... me too</td>\n",
       "      <td>neutral</td>\n",
       "      <td>... me too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3401</th>\n",
       "      <td>d3d8561006</td>\n",
       "      <td>Omg Beckky i love you! you should`ve won the 250G`s  i ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>you! you should</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2721</th>\n",
       "      <td>63d6ff064f</td>\n",
       "      <td>after 11 months....back to twitter again</td>\n",
       "      <td>neutral</td>\n",
       "      <td>months....back to twitter again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>84548ab6e2</td>\n",
       "      <td>Your the supporter  its totally up to you</td>\n",
       "      <td>neutral</td>\n",
       "      <td>supporter its totally up to you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2793</th>\n",
       "      <td>d01966504b</td>\n",
       "      <td>totally just got hit on by one of her bar guests! Ha ha....</td>\n",
       "      <td>neutral</td>\n",
       "      <td>got hit on by one of her bar guests! ha ha...yay me!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3374</th>\n",
       "      <td>a9b501f7cd</td>\n",
       "      <td>At the wedding reception. Having more fun than I thought</td>\n",
       "      <td>positive</td>\n",
       "      <td>fun than i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945</th>\n",
       "      <td>144e6fb3ad</td>\n",
       "      <td>lazy day, staying off the foot as much as possible.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>, staying off the foot as much as possible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>94e0366b6d</td>\n",
       "      <td>http://twitpic.com/4u5h8 - leon looks supa` fly on that...</td>\n",
       "      <td>positive</td>\n",
       "      <td>looks supa` fly on that</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "1253  b09213d54b   no. my school will start on June1.  two days to go. i s...   \n",
       "3063  9f23e5d8f5                                 someone doesn`t feel good...   \n",
       "172   5ff525b74b  Wondering if i cld make things any worse than they alrea...   \n",
       "3372  817b6da55c  Okay, so the only reason I`m not buying this app is beca...   \n",
       "2777  3593dbe193                                is all alone for the evening!   \n",
       "473   b56108fc36   hahaha niene ur soo smart.! lmao i think that is who it...   \n",
       "1404  f61ae5403a   It`s 12AM! ,  and I are still up, sending you as much t...   \n",
       "2689  1b4612b286  I have a job at camp!!  Only downfall? No midnight showi...   \n",
       "2846  44da94a9b7  Want to hang out with beth and tenaya and their hubbys t...   \n",
       "2905  416e6352b9                     are you in need of another kiss attack??   \n",
       "1636  7811012154           Not made it to work  couldn`t get up feelin blurgh   \n",
       "1835  0b25a31b7e                                                 is surfin`..   \n",
       "1254  79256fbe62                                      Still no Internet today   \n",
       "1861  706a860942                                    _tron oh god, whose tire?   \n",
       "1064  0b3588bf63  Happy Mother`s Day all of the mom`s around the world!  I...   \n",
       "3263  dec8c3dac3                                      HAPPY MOTHERS DAY MAMMA   \n",
       "560   527b27665d  Just Chilling after MCFLY last night! ABSOULUTELY INCRED...   \n",
       "1629  5f91766356                                                aww... me too   \n",
       "3401  d3d8561006   Omg Beckky i love you! you should`ve won the 250G`s  i ...   \n",
       "2721  63d6ff064f                     after 11 months....back to twitter again   \n",
       "2762  84548ab6e2                    Your the supporter  its totally up to you   \n",
       "2793  d01966504b  totally just got hit on by one of her bar guests! Ha ha....   \n",
       "3374  a9b501f7cd     At the wedding reception. Having more fun than I thought   \n",
       "2945  144e6fb3ad          lazy day, staying off the foot as much as possible.   \n",
       "2151  94e0366b6d   http://twitpic.com/4u5h8 - leon looks supa` fly on that...   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "1253  negative                                               don`t want. :l  \n",
       "3063  negative                                              `t feel good...  \n",
       "172   negative                                                         they  \n",
       "3372   neutral   so the only reason i`m not buying this app is because i...  \n",
       "2777   neutral                                       alone for the evening!  \n",
       "473   positive                                                    in.. whts  \n",
       "1404   neutral  s 12am! , and i are still up, sending you as much tweets...  \n",
       "2689   neutral   a job at camp!! only downfall? no midnight showing of h...  \n",
       "2846   neutral   hang out with beth and tenaya and their hubbys tonight....  \n",
       "2905   neutral                             in need of another kiss attack??  \n",
       "1636  negative                                       t get up feelin blurgh  \n",
       "1835   neutral                                                        in`..  \n",
       "1254   neutral                                               internet today  \n",
       "1861   neutral                                      ron oh god, whose tire?  \n",
       "1064  positive                                                            `  \n",
       "3263  positive                                                          day  \n",
       "560   positive                                         utely incredible! =d  \n",
       "1629   neutral                                                   ... me too  \n",
       "3401  positive                                              you! you should  \n",
       "2721   neutral                              months....back to twitter again  \n",
       "2762   neutral                              supporter its totally up to you  \n",
       "2793   neutral         got hit on by one of her bar guests! ha ha...yay me!  \n",
       "3374  positive                                                   fun than i  \n",
       "2945   neutral                  , staying off the foot as much as possible.  \n",
       "2151  positive                                      looks supa` fly on that  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
