{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XLNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu7jnIpK_K4a",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Installing transformers and tokenizers libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdh62EzCvNiw",
        "colab_type": "code",
        "outputId": "443d2173-0573-4093-ae40-7d733ce4eda9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        }
      },
      "source": [
        "!pip install tokenizers\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.7.0\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 13.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 16.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=a212ee1da97efedd00021f270f82f82e16ff20f89b618d226d79c1273c3e2944\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcAWwiYu_Xuv",
        "colab_type": "text"
      },
      "source": [
        "## Importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZSW-EozvW5B",
        "colab_type": "code",
        "outputId": "2cbcac03-94fa-42c7-ead6-503b310ad2f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from transformers import *\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import random\n",
        "from google.colab import drive\n",
        "import tensorflow.keras.backend as K\n",
        "import sentencepiece as spm\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import math"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHNyV7Mj_d4t",
        "colab_type": "text"
      },
      "source": [
        "## Mounting gdrive (if you are using colab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXdnh-39NAZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnUoh-ii_i6P",
        "colab_type": "text"
      },
      "source": [
        "## Importing tokenizer and xlnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXV0Fiy--Uyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased' ,\n",
        "                                           do_lower_case = True)\n",
        "xlnet = TFXLNetModel.from_pretrained('xlnet-base-cased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boj24KwXE7tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd '/content/gdrive'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTgTqnu3_uir",
        "colab_type": "text"
      },
      "source": [
        "## Reading training and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyrCy2cK4jV5",
        "colab_type": "code",
        "outputId": "ab35fede-7b04-4142-9816-186adbd216de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "train_dir = '/content/gdrive/My Drive/tweet sentiment/train.csv'\n",
        "test_dir = '/content/gdrive/My Drive/tweet sentiment/test.csv'\n",
        "train = pd.read_csv(train_dir).fillna(' ')\n",
        "test = pd.read_csv(test_dir).fillna(' ')\n",
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cb774db0d1</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
              "      <td>Sons of ****,</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID  ... sentiment\n",
              "0  cb774db0d1  ...   neutral\n",
              "1  549e992a42  ...  negative\n",
              "2  088c60f138  ...  negative\n",
              "3  9642c003ef  ...  negative\n",
              "4  358bd9e861  ...  negative\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnw4n4Ie5sf5",
        "colab_type": "code",
        "outputId": "c4b12818-24c3-4b54-a652-396fe62ef980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>f87dea47db</td>\n",
              "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>96d74cb729</td>\n",
              "      <td>Shanghai is also really exciting (precisely -...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>eee518ae67</td>\n",
              "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01082688c6</td>\n",
              "      <td>happy bday!</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33987a8ee5</td>\n",
              "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID                                               text sentiment\n",
              "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n",
              "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive\n",
              "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative\n",
              "3  01082688c6                                        happy bday!  positive\n",
              "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArDrj7Lm_1MT",
        "colab_type": "text"
      },
      "source": [
        "## priniting random tokenized sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOt_OWiM6qda",
        "colab_type": "code",
        "outputId": "b15c1408-e847-4b1c-c6eb-ea70ff1d359d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "k = random.randrange(train.shape[0])\n",
        "example = train.loc[k ,'text']\n",
        "enc = tokenizer.encode(example)\n",
        "print('statement is \\'{}\\''.format(example))\n",
        "print('encoding is {}'.format(enc))\n",
        "sentence = ''\n",
        "for en in enc:\n",
        "    token = tokenizer._convert_id_to_token(en)\n",
        "    print('{} : {}'.format(token ,en))\n",
        "    if token != '<sep>' and token != '<cls>':\n",
        "        sentence = sentence + token\n",
        "sentence = sentence.replace('▁' ,\" \").strip()\n",
        "print(sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "statement is ' Ryaaaaaaaaaaaaan  http://bit.ly/SnjEn'\n",
            "encoding is [17, 844, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 262, 4538, 4315, 2802, 9, 111, 167, 23, 16603, 254, 4, 3]\n",
            "▁ : 17\n",
            "ry : 844\n",
            "a : 101\n",
            "a : 101\n",
            "a : 101\n",
            "a : 101\n",
            "a : 101\n",
            "a : 101\n",
            "a : 101\n",
            "a : 101\n",
            "a : 101\n",
            "a : 101\n",
            "a : 101\n",
            "a : 101\n",
            "an : 262\n",
            "▁http : 4538\n",
            ":// : 4315\n",
            "bit : 2802\n",
            ". : 9\n",
            "ly : 111\n",
            "/ : 167\n",
            "s : 23\n",
            "nj : 16603\n",
            "en : 254\n",
            "<sep> : 4\n",
            "<cls> : 3\n",
            "ryaaaaaaaaaaaaan http://bit.ly/snjen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaRhJ8f-4gup",
        "colab_type": "code",
        "outputId": "44790240-6456-455d-e64a-bd72bdb16207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.encode('<cls>')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 4, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urLIi8gatvLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD_ID = 5\n",
        "SEED = 88888\n",
        "LABEL_SMOOTHING = 0.1\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tQpEu435203",
        "colab_type": "code",
        "outputId": "c3ce312a-2d4a-4de2-82b8-61f6a0a72901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "MAX_LEN = 100\n",
        "special_token = {'<sep>':4 ,'<cls>':3 ,'<pad>':5}\n",
        "positive = tokenizer.encode('positive')[:-2]\n",
        "negative = tokenizer.encode('negative')[:-2]\n",
        "neutral = tokenizer.encode('neutral')[:-2]\n",
        "sent_tokens  ={'positive': positive ,'negative':negative ,'neutral' : neutral}\n",
        "print(sent_tokens['positive'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1654]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2AU2E8d_8iI",
        "colab_type": "text"
      },
      "source": [
        "## Making up training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL0jYcDH54mT",
        "colab_type": "code",
        "outputId": "ce4b564b-d32c-4b2e-b267-292074e91905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "ct = train.shape[0]\n",
        "input_ids = np.zeros((ct ,MAX_LEN))\n",
        "attention_mask = np.zeros((ct ,MAX_LEN))\n",
        "token_type_ids = np.zeros((ct ,MAX_LEN))\n",
        "start_tokens = np.zeros((ct ,MAX_LEN))\n",
        "end_tokens = np.zeros((ct ,MAX_LEN))\n",
        "counter = 0\n",
        "for k in range(ct):\n",
        "    text = train.loc[k ,'text']\n",
        "    text = \" \" + \" \".join(text.split())\n",
        "    selected_text = train.loc[k ,'selected_text']\n",
        "    selected_text = \" \".join(selected_text.split())\n",
        "    sent = train.loc[k ,'sentiment']\n",
        "    text_enc = tokenizer.encode(text)[:-2]\n",
        "    selected_enc = tokenizer.encode(selected_text)[:-2]\n",
        "    idx = text.find(selected_text)\n",
        "    chars = np.zeros((len(text)))\n",
        "    chars[idx:idx+len(selected_text)] = 1\n",
        "    if text[idx-1] == ' ':chars[idx-1] =1\n",
        "    \n",
        "    offsets = []\n",
        "    idx = 0\n",
        "    for en in text_enc:\n",
        "        token = tokenizer._convert_id_to_token(en)\n",
        "        offsets.append((idx ,idx+len(token)))\n",
        "        idx += len(token)\n",
        "    toks = []\n",
        "    for i ,(a ,b) in enumerate(offsets):\n",
        "        if np.sum(chars[a : b]) >0 :\n",
        "            toks.append(i)\n",
        "    sp = special_token['<sep>']\n",
        "    cl = special_token['<cls>']\n",
        "    pad = special_token['<pad>']\n",
        "    enc_final =  [cl] + sent_tokens[sent] + [sp ,sp] + text_enc + [sp] \n",
        "    input_ids[k ,:] = enc_final + (MAX_LEN - len(enc_final))*[PAD_ID]\n",
        "    attention_mask[k ,:] = len(enc_final)*[1] + (MAX_LEN-len(enc_final))*[0]\n",
        "    token_type_ids[k ,3:len(enc_final)] = 1\n",
        "    \n",
        "    if len(toks)>0:\n",
        "        start_tokens[k ,toks[0]+4] = 1\n",
        "        end_tokens[k ,toks[-1]+4] = 1 \n",
        "    if k == 2:\n",
        "        print(start_tokens[k])\n",
        "        print(end_tokens[k])\n",
        "        print(text)\n",
        "        print(input_ids[k])\n",
        "        print(selected_text)\n",
        "        a = np.argmax(start_tokens[k])\n",
        "        b = np.argmax(end_tokens[k])\n",
        "        man = tokenizer.encode(text)[:-2]\n",
        "        print(tokenizer.decode(man[a-4:b-3]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            " my boss is bullying me...\n",
            "[3.0000e+00 2.9810e+03 4.0000e+00 4.0000e+00 9.4000e+01 5.6430e+03\n",
            " 2.7000e+01 2.3175e+04 1.1000e+02 9.0000e+00 9.0000e+00 9.0000e+00\n",
            " 4.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00\n",
            " 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00\n",
            " 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00\n",
            " 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00\n",
            " 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00\n",
            " 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00\n",
            " 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00\n",
            " 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00\n",
            " 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00\n",
            " 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00\n",
            " 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00\n",
            " 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00\n",
            " 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00\n",
            " 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00\n",
            " 5.0000e+00 5.0000e+00 5.0000e+00 5.0000e+00]\n",
            "bullying me\n",
            "bullying me\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Cg-xKLA5-Se",
        "colab_type": "code",
        "outputId": "c409cf25-f529-4f4c-9a6b-cea4f5dc5df6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "print(train.loc[314 ,'text'])\n",
        "print(input_ids[314])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            "[3.000e+00 9.201e+03 4.000e+00 4.000e+00 4.000e+00 5.000e+00 5.000e+00\n",
            " 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00\n",
            " 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00\n",
            " 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00\n",
            " 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00\n",
            " 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00\n",
            " 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00\n",
            " 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00\n",
            " 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00\n",
            " 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00\n",
            " 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00\n",
            " 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00\n",
            " 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00\n",
            " 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00 5.000e+00\n",
            " 5.000e+00 5.000e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZZyDDyKAIjl",
        "colab_type": "text"
      },
      "source": [
        "## Making up test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga0Zk64R6B2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct_t = test.shape[0]\n",
        "input_ids_t = np.zeros((ct_t ,MAX_LEN))\n",
        "attention_mask_t = np.zeros((ct_t ,MAX_LEN))\n",
        "token_type_ids_t = np.zeros((ct_t ,MAX_LEN))\n",
        "for k in range(ct_t):\n",
        "    text = test.loc[k ,'text']\n",
        "    text = \" \" + \" \".join(text.split())\n",
        "    sent = test.loc[k ,'sentiment']\n",
        "    text_enc = tokenizer.encode(text)[:-2]\n",
        "    sp = special_token['<sep>']\n",
        "    cl = special_token['<cls>']\n",
        "    pad = special_token['<pad>']\n",
        "    enc_final = [cl] + sent_tokens[sent] + [sp ,sp] + text_enc + [sp] \n",
        "    input_ids_t[k ,:] = enc_final + (MAX_LEN - len(enc_final))*[PAD_ID]\n",
        "    attention_mask_t[k ,:] = len(enc_final)*[1] + (MAX_LEN-len(enc_final))*[0]\n",
        "    token_type_ids_t[k ,3:len(enc_final)] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJN-FUVIAA0k",
        "colab_type": "text"
      },
      "source": [
        "## Building Model , loss ,metric and some helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "943Hgc2v6JiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_weights(model, dst_fn):\n",
        "    weights = model.get_weights()\n",
        "    with open(dst_fn, 'wb') as f:\n",
        "        pickle.dump(weights, f)\n",
        "\n",
        "\n",
        "def load_weights(model, weight_fn):\n",
        "    with open(weight_fn, 'rb') as f:\n",
        "        weights = pickle.load(f)\n",
        "    model.set_weights(weights)\n",
        "    return model\n",
        "\n",
        "def loss_fn(y_true, y_pred):\n",
        "    # adjust the targets for sequence bucketing\n",
        "    ll = tf.shape(y_pred)[1]\n",
        "    y_true = y_true[:, :ll]\n",
        "    loss = tf.keras.losses.binary_crossentropy(y_true, y_pred,\n",
        "        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss\n",
        "def loss(gamma):\n",
        "    def myloss(y_true ,y_pred):\n",
        "        ll = tf.shape(y_pred)[1]\n",
        "        y_true = y_true[:, :ll]\n",
        "        loss = ((1-y_pred)**gamma)*y_true*K.log(y_pred) + (y_pred**gamma)*(1-y_true)*K.log(1-y_pred)\n",
        "        return -tf.reduce_mean(loss)\n",
        "    return myloss\n",
        "def build_model():\n",
        "    tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased' ,do_lower_case = True)\n",
        "    xlnet = TFXLNetModel.from_pretrained('xlnet-base-cased')\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
        "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
        "    max_len = tf.reduce_max(lens)\n",
        "    ids_ = ids[:, :max_len]\n",
        "    att_ = att[:, :max_len]\n",
        "    tok_ = tok[:, :max_len]\n",
        "    x = xlnet(ids_,attention_mask=att_,token_type_ids=tok_)\n",
        "    \n",
        "    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "    model = tf.keras.models.Model(inputs = [ids ,att ,tok] ,outputs = [x1 ,x2])\n",
        "    print('here1')\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n",
        "    model.compile(loss=loss(1.5), optimizer=optimizer)\n",
        "    print('here2')\n",
        "    # this is required as `model.predict` needs a fixed size!\n",
        "    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.0)\n",
        "    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.0)\n",
        "    print('here3')\n",
        "    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n",
        "    return model ,padded_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDtjnBHy6tNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    if (len(a)==0) & (len(b)==0): return 0.5\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMX1g6xCANdU",
        "colab_type": "text"
      },
      "source": [
        "## Training Phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNZ3uZDy7a_B",
        "colab_type": "code",
        "outputId": "b2f82b28-cc3d-490b-d7ab-7d687cdaac76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        }
      },
      "source": [
        "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
        "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "EPOCHS = 4\n",
        "LABEL_SMOOTHING = 0.1\n",
        "BATCH_SIZE = 32\n",
        "skf = StratifiedKFold(n_splits=5,shuffle=True ,random_state=SEED) #originally 5 splits\n",
        "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
        "\n",
        "    print('#'*25)\n",
        "    print('### FOLD %i'%(fold+1))\n",
        "    print('#'*25)\n",
        "    \n",
        "    K.clear_session()\n",
        "    model, padded_model = build_model()\n",
        "        \n",
        "    #sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "    #    '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
        "    #    save_weights_only=True, mode='auto', save_freq='epoch')\n",
        "    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n",
        "    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n",
        "    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n",
        "    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n",
        "    # sort the validation data\n",
        "    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n",
        "    inpV = [arr[shuffleV] for arr in inpV]\n",
        "    targetV = [arr[shuffleV] for arr in targetV]\n",
        "    weight_fn = '%s-XLNet-%i.h5'%(VER,fold)\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        # sort and shuffle: We add random numbers to not have the same order in each epoch\n",
        "        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == 5.0).sum() + np.random.randint(-3, 3), reverse=True))\n",
        "        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n",
        "        num_batches = math.ceil(len(shuffleT) / BATCH_SIZE)\n",
        "        batch_inds = np.random.permutation(num_batches)\n",
        "        shuffleT_ = []\n",
        "        for batch_ind in batch_inds:\n",
        "            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n",
        "        shuffleT = np.concatenate(shuffleT_)\n",
        "        # reorder the input data\n",
        "        inpT = [arr[shuffleT] for arr in inpT]\n",
        "        targetT = [arr[shuffleT] for arr in targetT]\n",
        "        model.fit(inpT, targetT, \n",
        "            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[],\n",
        "            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n",
        "        save_weights(model, '/content/gdrive/My Drive/' + weight_fn)\n",
        "\n",
        "    print('Loading model...')\n",
        "    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
        "    load_weights(model, '/content/gdrive/My Drive/' + weight_fn)\n",
        "\n",
        "    print('Predicting OOF...')\n",
        "    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
        "    \n",
        "    print('Predicting Test...')\n",
        "    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
        "    preds_start += preds[0]/skf.n_splits\n",
        "    preds_end += preds[1]/skf.n_splits\n",
        "    \n",
        "    # DISPLAY FOLD JACCARD\n",
        "    all = []\n",
        "    for k in idxV:\n",
        "        a = np.argmax(oof_start[k,])\n",
        "        b = np.argmax(oof_end[k,])\n",
        "        if a>b: \n",
        "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
        "        else:\n",
        "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
        "            enc = tokenizer.encode(text1)[:-2]\n",
        "            st = tokenizer.decode(enc[a-4:b-3])\n",
        "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
        "    jac.append(np.mean(all))\n",
        "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#########################\n",
            "### FOLD 1\n",
            "#########################\n",
            "here1\n",
            "here2\n",
            "here3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
            "687/687 [==============================] - 128s 186ms/step - loss: 0.0863 - activation_loss: 0.0435 - activation_1_loss: 0.0428 - val_loss: 0.0594 - val_activation_loss: 0.0296 - val_activation_1_loss: 0.0299\n",
            "Epoch 2/2\n",
            "687/687 [==============================] - 125s 181ms/step - loss: 0.0546 - activation_loss: 0.0274 - activation_1_loss: 0.0272 - val_loss: 0.0555 - val_activation_loss: 0.0279 - val_activation_1_loss: 0.0276\n",
            "Epoch 3/3\n",
            "687/687 [==============================] - 125s 181ms/step - loss: 0.0478 - activation_loss: 0.0240 - activation_1_loss: 0.0237 - val_loss: 0.0538 - val_activation_loss: 0.0267 - val_activation_1_loss: 0.0271\n",
            "Epoch 4/4\n",
            "687/687 [==============================] - 125s 182ms/step - loss: 0.0429 - activation_loss: 0.0218 - activation_1_loss: 0.0211 - val_loss: 0.0546 - val_activation_loss: 0.0271 - val_activation_1_loss: 0.0275\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 17s 97ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 11s 95ms/step\n",
            ">>>> FOLD 1 Jaccard = 0.7018650389643601\n",
            "\n",
            "#########################\n",
            "### FOLD 2\n",
            "#########################\n",
            "here1\n",
            "here2\n",
            "here3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
            "118/688 [====>.........................] - ETA: 1:30 - loss: 0.1445 - activation_loss: 0.0771 - activation_1_loss: 0.0673"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FIMP_fmD729",
        "colab_type": "code",
        "outputId": "f03f7655-b8b9-44da-da09-63220a20ca84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(jac)\n",
        "print('mean jac {}'.format(sum(jac)/len(jac)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6985422242177061, 0.7037288150573154, 0.708220014204697, 0.6951575595107675, 0.6999276029364913]\n",
            "mean jac 0.7011152431853954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJB6XPYlAURt",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mX5BLy37k1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all = []\n",
        "for k in range(input_ids_t.shape[0]):\n",
        "    a = np.argmax(preds_start[k,])\n",
        "    b = np.argmax(preds_end[k,])\n",
        "    if a>b: \n",
        "        st = test.loc[k,'text']\n",
        "    else:\n",
        "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
        "        enc = tokenizer.encode(text1)\n",
        "        st = tokenizer.decode(enc[a-2:b-1])\n",
        "    all.append(st)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAfJgHndotud",
        "colab_type": "code",
        "outputId": "91808af7-7b64-4c64-f712-e55bf30fdab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        }
      },
      "source": [
        "test['selected_text'] = all\n",
        "test[['textID','selected_text']].to_csv('/content/gdrive/My Drive/tweet sentiment/submission.csv',index=False)\n",
        "pd.set_option('max_colwidth', 60)\n",
        "test.sample(25)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>selected_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>866</th>\n",
              "      <td>41a96b7954</td>\n",
              "      <td>me because I might not have enough money for college!!!</td>\n",
              "      <td>negative</td>\n",
              "      <td>i might not have enough money for college!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2123</th>\n",
              "      <td>3da6270b02</td>\n",
              "      <td>I hide my berry like a slave REGULARLY only today I was...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>i hide my berry like a slave regularly only today i was ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3355</th>\n",
              "      <td>32cde6dbc5</td>\n",
              "      <td>in school w. linda doing nothing  ;i miss you</td>\n",
              "      <td>negative</td>\n",
              "      <td>miss you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2599</th>\n",
              "      <td>12ceb7565a</td>\n",
              "      <td>Outlook not so good</td>\n",
              "      <td>negative</td>\n",
              "      <td>not so good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1169</th>\n",
              "      <td>7240ccb4b2</td>\n",
              "      <td>True to form, Bank Holiday Monday looks like it might be...</td>\n",
              "      <td>positive</td>\n",
              "      <td>hope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2857</th>\n",
              "      <td>16fd300910</td>\n",
              "      <td>Ive been passed out drunk for the passed couple of hours...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>ive been passed out drunk for the passed couple of hours...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2071</th>\n",
              "      <td>e0264b2a5f</td>\n",
              "      <td>I miss you</td>\n",
              "      <td>negative</td>\n",
              "      <td>i miss you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>415</th>\n",
              "      <td>52ad93b3fe</td>\n",
              "      <td>Oh No!!!! I must be gettin old!!!! My mom use to watch t...</td>\n",
              "      <td>negative</td>\n",
              "      <td>miss</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1768</th>\n",
              "      <td>6c18eab109</td>\n",
              "      <td>I cant change my profile picture on Facebook</td>\n",
              "      <td>neutral</td>\n",
              "      <td>i cant change my profile picture on facebook</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>5aa8a5280f</td>\n",
              "      <td>I knooww  &amp; my hot water bottle iss in whangamata witho...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>i knooww &amp; my hot water bottle iss in whangamata without...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3351</th>\n",
              "      <td>1305139c4a</td>\n",
              "      <td>_kikireestl nooo. you were on my yahoo account. hmm. i w...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>_kikireestl nooo. you were on my yahoo account. hmm. i w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2337</th>\n",
              "      <td>5055018a65</td>\n",
              "      <td>when u go, LMK. Let`s go together</td>\n",
              "      <td>positive</td>\n",
              "      <td>lmk. let`s go together</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2813</th>\n",
              "      <td>b40a435ea7</td>\n",
              "      <td>Let me check, Sirs</td>\n",
              "      <td>neutral</td>\n",
              "      <td>let me check, sirs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3517</th>\n",
              "      <td>558fe1316b</td>\n",
              "      <td>On a 10min brake. At wrrkk ... its 11:06 and its over at...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>on a 10min brake. at wrrkk... its 11:06 and its over at....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1634</th>\n",
              "      <td>4f9ee6208c</td>\n",
              "      <td>_city tell brandon I said hi!!</td>\n",
              "      <td>neutral</td>\n",
              "      <td>_city tell brandon i said hi!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>908</th>\n",
              "      <td>03e41eeb4f</td>\n",
              "      <td>thanks Alen, it seems that we can only submit community...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>thanks alen, it seems that we can only submit community ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>bcb9426abb</td>\n",
              "      <td>I wish I could go to #BEA this weekend.</td>\n",
              "      <td>positive</td>\n",
              "      <td>wish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1191</th>\n",
              "      <td>12005b65fc</td>\n",
              "      <td>Waiting for my turn on wii fit gym closed</td>\n",
              "      <td>neutral</td>\n",
              "      <td>waiting for my turn on wii fit gym closed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2443</th>\n",
              "      <td>1c5ca67cf9</td>\n",
              "      <td>Good Grief! I can`t say much, I was driving home from M...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>good grief! i can`t say much, i was driving home from ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>637</th>\n",
              "      <td>1a82a50aa9</td>\n",
              "      <td>I send messages to greg all the time, no answer  its ok...</td>\n",
              "      <td>positive</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2910</th>\n",
              "      <td>bd095b4751</td>\n",
              "      <td>Tweetioi in class is a real problem  =Taylor=</td>\n",
              "      <td>negative</td>\n",
              "      <td>problem</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>817</th>\n",
              "      <td>312868ebb2</td>\n",
              "      <td>hello you!  you should totally go to the biggest shoppi...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>hello you! you should totally go to the biggest shopping...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2498</th>\n",
              "      <td>1caf64ff52</td>\n",
              "      <td>_iscool I have a 320GB drive in my MB, with 6GB free... ...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>_iscool i have a 320gb drive in my mb, with 6gb free... ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3064</th>\n",
              "      <td>c7a59f50c3</td>\n",
              "      <td>Good Morning people!!! Have a great day</td>\n",
              "      <td>positive</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1116</th>\n",
              "      <td>a3b686ba61</td>\n",
              "      <td>tomorrow`s mother`s day  me n my sis gonna make an onigi...</td>\n",
              "      <td>positive</td>\n",
              "      <td>cant wait for tomorrow</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          textID  ...                                                selected_text\n",
              "866   41a96b7954  ...                 i might not have enough money for college!!!\n",
              "2123  3da6270b02  ...  i hide my berry like a slave regularly only today i was ...\n",
              "3355  32cde6dbc5  ...                                                     miss you\n",
              "2599  12ceb7565a  ...                                                  not so good\n",
              "1169  7240ccb4b2  ...                                                         hope\n",
              "2857  16fd300910  ...  ive been passed out drunk for the passed couple of hours...\n",
              "2071  e0264b2a5f  ...                                                   i miss you\n",
              "415   52ad93b3fe  ...                                                         miss\n",
              "1768  6c18eab109  ...                 i cant change my profile picture on facebook\n",
              "323   5aa8a5280f  ...  i knooww & my hot water bottle iss in whangamata without...\n",
              "3351  1305139c4a  ...  _kikireestl nooo. you were on my yahoo account. hmm. i w...\n",
              "2337  5055018a65  ...                                       lmk. let`s go together\n",
              "2813  b40a435ea7  ...                                           let me check, sirs\n",
              "3517  558fe1316b  ...  on a 10min brake. at wrrkk... its 11:06 and its over at....\n",
              "1634  4f9ee6208c  ...                               _city tell brandon i said hi!!\n",
              "908   03e41eeb4f  ...  thanks alen, it seems that we can only submit community ...\n",
              "137   bcb9426abb  ...                                                         wish\n",
              "1191  12005b65fc  ...                    waiting for my turn on wii fit gym closed\n",
              "2443  1c5ca67cf9  ...  good grief! i can`t say much, i was driving home from ma...\n",
              "637   1a82a50aa9  ...                                                         love\n",
              "2910  bd095b4751  ...                                                      problem\n",
              "817   312868ebb2  ...  hello you! you should totally go to the biggest shopping...\n",
              "2498  1caf64ff52  ...  _iscool i have a 320gb drive in my mb, with 6gb free... ...\n",
              "3064  c7a59f50c3  ...                                                         good\n",
              "1116  a3b686ba61  ...                                       cant wait for tomorrow\n",
              "\n",
              "[25 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-assscOMox-9",
        "colab_type": "code",
        "outputId": "8902ec51-42c6-4b4c-95bc-020afcd55876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        }
      },
      "source": [
        "test.sample(25)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>selected_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3428</th>\n",
              "      <td>e4c90cafae</td>\n",
              "      <td>I`m mo nudge you again, better watch out!</td>\n",
              "      <td>negative</td>\n",
              "      <td>better watch out!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1220</th>\n",
              "      <td>aa221b6a7d</td>\n",
              "      <td>hey, what about us followers in ATL!!!!</td>\n",
              "      <td>neutral</td>\n",
              "      <td>hey, what about us followers in atl!!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2611</th>\n",
              "      <td>e2c48f0201</td>\n",
              "      <td>Slept in, woke up with an iced coffee, lazed about &amp; wen...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>slept in, woke up with an iced coffee, lazed about &amp; wen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3111</th>\n",
              "      <td>c780b84d97</td>\n",
              "      <td>yea - it`s mostly b/c I couldn`t sleep, but oh well, lu...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>yea - it`s mostly b/c i couldn`t sleep, but oh well, lun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3292</th>\n",
              "      <td>a35314257a</td>\n",
              "      <td>Edgefest!  or maybe  since you`re driving...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>edgefest! or maybe since you`re driving...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>605</th>\n",
              "      <td>9a0fce6d7e</td>\n",
              "      <td>learning how to use twitter</td>\n",
              "      <td>neutral</td>\n",
              "      <td>learning how to use twitter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2151</th>\n",
              "      <td>94e0366b6d</td>\n",
              "      <td>http://twitpic.com/4u5h8 - leon looks supa` fly on that...</td>\n",
              "      <td>positive</td>\n",
              "      <td>leon looks supa`</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2857</th>\n",
              "      <td>16fd300910</td>\n",
              "      <td>Ive been passed out drunk for the passed couple of hours...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>ive been passed out drunk for the passed couple of hours...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2031</th>\n",
              "      <td>b303da170c</td>\n",
              "      <td>I am going to die tomorrow night.   should be here.</td>\n",
              "      <td>negative</td>\n",
              "      <td>die</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>8f0bafc9dc</td>\n",
              "      <td>In effect, your podcast IS the audio version of your ...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>in effect, your podcast is the audio version of your boo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2179</th>\n",
              "      <td>08d8063529</td>\n",
              "      <td>Had a good time out , now I must sleep lol. Gym &amp; dance ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>963</th>\n",
              "      <td>d3935b8571</td>\n",
              "      <td>It`s surprising how much Billy Idol turns up in tweets....</td>\n",
              "      <td>neutral</td>\n",
              "      <td>it`s surprising how much billy idol turns up in tweets. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>32ee7ef6c2</td>\n",
              "      <td>Hi all, just recovering from a party, looking forward to...</td>\n",
              "      <td>positive</td>\n",
              "      <td>looking forward to an exciting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2509</th>\n",
              "      <td>b9b4e92118</td>\n",
              "      <td>Back at work</td>\n",
              "      <td>neutral</td>\n",
              "      <td>back at work</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>452</th>\n",
              "      <td>d35f126ad0</td>\n",
              "      <td>Leaving the beach .. Having a great day with vicente .. ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>great</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1326</th>\n",
              "      <td>a193f3b9e1</td>\n",
              "      <td>NO! Keyboard Cat is/has been dead for YEARS! Play yourse...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>no! keyboard cat is/has been dead for years! play yourse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1435</th>\n",
              "      <td>98cfd1162e</td>\n",
              "      <td>aww...thats a bummer</td>\n",
              "      <td>negative</td>\n",
              "      <td>bummer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>779</th>\n",
              "      <td>0ee55890a7</td>\n",
              "      <td>Geeze- I`m just wondering what is wrong with some people...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>geeze- i`m just wondering what is wrong with some people...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1631</th>\n",
              "      <td>2d5b920414</td>\n",
              "      <td>I`ll have to wait for the YouTube links</td>\n",
              "      <td>neutral</td>\n",
              "      <td>i`ll have to wait for the youtube links</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>459</th>\n",
              "      <td>66f1863628</td>\n",
              "      <td>I want chocolate!</td>\n",
              "      <td>neutral</td>\n",
              "      <td>i want chocolate!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2519</th>\n",
              "      <td>1115490075</td>\n",
              "      <td>_Day26 awww man, its nott</td>\n",
              "      <td>neutral</td>\n",
              "      <td>_day26 awww man, its nott</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2418</th>\n",
              "      <td>49c713c76d</td>\n",
              "      <td>Is It The Bit Where Hollie Started Crying?</td>\n",
              "      <td>neutral</td>\n",
              "      <td>is it the bit where hollie started crying?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1908</th>\n",
              "      <td>6788d6c565</td>\n",
              "      <td>Aww the little girl on Britain`s Got Talent!  I actually...</td>\n",
              "      <td>positive</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>58f1b52257</td>\n",
              "      <td>i want to gooo</td>\n",
              "      <td>positive</td>\n",
              "      <td>i want to gooo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>0f89951346</td>\n",
              "      <td>No launch today. Teacher changed plans and we watched a ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>stupid</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          textID  ...                                                selected_text\n",
              "3428  e4c90cafae  ...                                            better watch out!\n",
              "1220  aa221b6a7d  ...                      hey, what about us followers in atl!!!!\n",
              "2611  e2c48f0201  ...  slept in, woke up with an iced coffee, lazed about & wen...\n",
              "3111  c780b84d97  ...  yea - it`s mostly b/c i couldn`t sleep, but oh well, lun...\n",
              "3292  a35314257a  ...                   edgefest! or maybe since you`re driving...\n",
              "605   9a0fce6d7e  ...                                  learning how to use twitter\n",
              "2151  94e0366b6d  ...                                             leon looks supa`\n",
              "2857  16fd300910  ...  ive been passed out drunk for the passed couple of hours...\n",
              "2031  b303da170c  ...                                                          die\n",
              "300   8f0bafc9dc  ...  in effect, your podcast is the audio version of your boo...\n",
              "2179  08d8063529  ...                                                         good\n",
              "963   d3935b8571  ...  it`s surprising how much billy idol turns up in tweets. ...\n",
              "66    32ee7ef6c2  ...                               looking forward to an exciting\n",
              "2509  b9b4e92118  ...                                                 back at work\n",
              "452   d35f126ad0  ...                                                        great\n",
              "1326  a193f3b9e1  ...  no! keyboard cat is/has been dead for years! play yourse...\n",
              "1435  98cfd1162e  ...                                                       bummer\n",
              "779   0ee55890a7  ...  geeze- i`m just wondering what is wrong with some people...\n",
              "1631  2d5b920414  ...                      i`ll have to wait for the youtube links\n",
              "459   66f1863628  ...                                            i want chocolate!\n",
              "2519  1115490075  ...                                    _day26 awww man, its nott\n",
              "2418  49c713c76d  ...                   is it the bit where hollie started crying?\n",
              "1908  6788d6c565  ...                                                         love\n",
              "438   58f1b52257  ...                                               i want to gooo\n",
              "220   0f89951346  ...                                                       stupid\n",
              "\n",
              "[25 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    }
  ]
}